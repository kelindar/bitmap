// Code generated by command: go run amd64.go -out ../simd_amd64.s -stubs ../simd_amd64.go -pkg=bitmap. DO NOT EDIT.

//go:build !appengine && !noasm && gc

#include "textflag.h"

// func x64and1(a []uint64, b []uint64)
// Requires: AVX, AVX2
TEXT ·x64and1(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ b_len+32(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "AND" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "AND" operation
	MOVQ (CX), BX
	ANDQ (AX), BX
	MOVQ BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET

// func x64andn1(a []uint64, b []uint64)
// Requires: AVX, AVX2, BMI
TEXT ·x64andn1(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ b_len+32(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "ANDN" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "ANDN" operation
	MOVQ  (CX), BX
	ANDNQ (AX), BX, BX
	MOVQ  BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET

// func x64or1(a []uint64, b []uint64)
// Requires: AVX, AVX2
TEXT ·x64or1(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ b_len+32(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "OR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "OR" operation
	MOVQ (CX), BX
	ORQ  (AX), BX
	MOVQ BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET

// func x64xor1(a []uint64, b []uint64)
// Requires: AVX, AVX2
TEXT ·x64xor1(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ b_len+32(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "XOR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "XOR" operation
	MOVQ (CX), BX
	XORQ (AX), BX
	MOVQ BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET

// func x64and2(a []uint64, b []uint64, c []uint64)
// Requires: AVX, AVX2
TEXT ·x64and2(SB), NOSPLIT, $0-72
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ b_len+32(FP), BX
	XORQ SI, SI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ BX, $0x00000008
	JL   tail

	// perform the logical "AND" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	SUBQ $0x00000008, BX
	JMP  body

tail:
	CMPQ BX, $0x00
	JE   done

	// perform the logical "AND" operation
	MOVQ (CX), SI
	ANDQ (AX), SI
	MOVQ SI, (AX)
	MOVQ (DX), SI
	ANDQ (AX), SI
	MOVQ SI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	SUBQ $0x00000001, BX
	JMP  tail

done:
	RET

// func x64andn2(a []uint64, b []uint64, c []uint64)
// Requires: AVX, AVX2, BMI
TEXT ·x64andn2(SB), NOSPLIT, $0-72
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ b_len+32(FP), BX
	XORQ SI, SI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ BX, $0x00000008
	JL   tail

	// perform the logical "ANDN" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	SUBQ $0x00000008, BX
	JMP  body

tail:
	CMPQ BX, $0x00
	JE   done

	// perform the logical "ANDN" operation
	MOVQ  (CX), SI
	ANDNQ (AX), SI, SI
	MOVQ  SI, (AX)
	MOVQ  (DX), SI
	ANDNQ (AX), SI, SI
	MOVQ  SI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	SUBQ $0x00000001, BX
	JMP  tail

done:
	RET

// func x64or2(a []uint64, b []uint64, c []uint64)
// Requires: AVX, AVX2
TEXT ·x64or2(SB), NOSPLIT, $0-72
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ b_len+32(FP), BX
	XORQ SI, SI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ BX, $0x00000008
	JL   tail

	// perform the logical "OR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	SUBQ $0x00000008, BX
	JMP  body

tail:
	CMPQ BX, $0x00
	JE   done

	// perform the logical "OR" operation
	MOVQ (CX), SI
	ORQ  (AX), SI
	MOVQ SI, (AX)
	MOVQ (DX), SI
	ORQ  (AX), SI
	MOVQ SI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	SUBQ $0x00000001, BX
	JMP  tail

done:
	RET

// func x64xor2(a []uint64, b []uint64, c []uint64)
// Requires: AVX, AVX2
TEXT ·x64xor2(SB), NOSPLIT, $0-72
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ b_len+32(FP), BX
	XORQ SI, SI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ BX, $0x00000008
	JL   tail

	// perform the logical "XOR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	SUBQ $0x00000008, BX
	JMP  body

tail:
	CMPQ BX, $0x00
	JE   done

	// perform the logical "XOR" operation
	MOVQ (CX), SI
	XORQ (AX), SI
	MOVQ SI, (AX)
	MOVQ (DX), SI
	XORQ (AX), SI
	MOVQ SI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	SUBQ $0x00000001, BX
	JMP  tail

done:
	RET

// func x64and3(a []uint64, b []uint64, c []uint64, d []uint64)
// Requires: AVX, AVX2
TEXT ·x64and3(SB), NOSPLIT, $0-96
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ b_len+32(FP), SI
	XORQ DI, DI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ SI, $0x00000008
	JL   tail

	// perform the logical "AND" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	SUBQ $0x00000008, SI
	JMP  body

tail:
	CMPQ SI, $0x00
	JE   done

	// perform the logical "AND" operation
	MOVQ (CX), DI
	ANDQ (AX), DI
	MOVQ DI, (AX)
	MOVQ (DX), DI
	ANDQ (AX), DI
	MOVQ DI, (AX)
	MOVQ (BX), DI
	ANDQ (AX), DI
	MOVQ DI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	SUBQ $0x00000001, SI
	JMP  tail

done:
	RET

// func x64andn3(a []uint64, b []uint64, c []uint64, d []uint64)
// Requires: AVX, AVX2, BMI
TEXT ·x64andn3(SB), NOSPLIT, $0-96
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ b_len+32(FP), SI
	XORQ DI, DI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ SI, $0x00000008
	JL   tail

	// perform the logical "ANDN" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	SUBQ $0x00000008, SI
	JMP  body

tail:
	CMPQ SI, $0x00
	JE   done

	// perform the logical "ANDN" operation
	MOVQ  (CX), DI
	ANDNQ (AX), DI, DI
	MOVQ  DI, (AX)
	MOVQ  (DX), DI
	ANDNQ (AX), DI, DI
	MOVQ  DI, (AX)
	MOVQ  (BX), DI
	ANDNQ (AX), DI, DI
	MOVQ  DI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	SUBQ $0x00000001, SI
	JMP  tail

done:
	RET

// func x64or3(a []uint64, b []uint64, c []uint64, d []uint64)
// Requires: AVX, AVX2
TEXT ·x64or3(SB), NOSPLIT, $0-96
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ b_len+32(FP), SI
	XORQ DI, DI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ SI, $0x00000008
	JL   tail

	// perform the logical "OR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	SUBQ $0x00000008, SI
	JMP  body

tail:
	CMPQ SI, $0x00
	JE   done

	// perform the logical "OR" operation
	MOVQ (CX), DI
	ORQ  (AX), DI
	MOVQ DI, (AX)
	MOVQ (DX), DI
	ORQ  (AX), DI
	MOVQ DI, (AX)
	MOVQ (BX), DI
	ORQ  (AX), DI
	MOVQ DI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	SUBQ $0x00000001, SI
	JMP  tail

done:
	RET

// func x64xor3(a []uint64, b []uint64, c []uint64, d []uint64)
// Requires: AVX, AVX2
TEXT ·x64xor3(SB), NOSPLIT, $0-96
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ b_len+32(FP), SI
	XORQ DI, DI

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ SI, $0x00000008
	JL   tail

	// perform the logical "XOR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	SUBQ $0x00000008, SI
	JMP  body

tail:
	CMPQ SI, $0x00
	JE   done

	// perform the logical "XOR" operation
	MOVQ (CX), DI
	XORQ (AX), DI
	MOVQ DI, (AX)
	MOVQ (DX), DI
	XORQ (AX), DI
	MOVQ DI, (AX)
	MOVQ (BX), DI
	XORQ (AX), DI
	MOVQ DI, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	SUBQ $0x00000001, SI
	JMP  tail

done:
	RET

// func x64and4(a []uint64, b []uint64, c []uint64, d []uint64, e []uint64)
// Requires: AVX, AVX2
TEXT ·x64and4(SB), NOSPLIT, $0-120
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ e_base+96(FP), SI
	MOVQ b_len+32(FP), DI
	XORQ R8, R8

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DI, $0x00000008
	JL   tail

	// perform the logical "AND" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (SI), Y0
	VMOVUPD 32(SI), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	ADDQ $0x00000040, SI
	SUBQ $0x00000008, DI
	JMP  body

tail:
	CMPQ DI, $0x00
	JE   done

	// perform the logical "AND" operation
	MOVQ (CX), R8
	ANDQ (AX), R8
	MOVQ R8, (AX)
	MOVQ (DX), R8
	ANDQ (AX), R8
	MOVQ R8, (AX)
	MOVQ (BX), R8
	ANDQ (AX), R8
	MOVQ R8, (AX)
	MOVQ (SI), R8
	ANDQ (AX), R8
	MOVQ R8, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	ADDQ $0x00000008, SI
	SUBQ $0x00000001, DI
	JMP  tail

done:
	RET

// func x64andn4(a []uint64, b []uint64, c []uint64, d []uint64, e []uint64)
// Requires: AVX, AVX2, BMI
TEXT ·x64andn4(SB), NOSPLIT, $0-120
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ e_base+96(FP), SI
	MOVQ b_len+32(FP), DI
	XORQ R8, R8

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DI, $0x00000008
	JL   tail

	// perform the logical "ANDN" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (SI), Y0
	VMOVUPD 32(SI), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	ADDQ $0x00000040, SI
	SUBQ $0x00000008, DI
	JMP  body

tail:
	CMPQ DI, $0x00
	JE   done

	// perform the logical "ANDN" operation
	MOVQ  (CX), R8
	ANDNQ (AX), R8, R8
	MOVQ  R8, (AX)
	MOVQ  (DX), R8
	ANDNQ (AX), R8, R8
	MOVQ  R8, (AX)
	MOVQ  (BX), R8
	ANDNQ (AX), R8, R8
	MOVQ  R8, (AX)
	MOVQ  (SI), R8
	ANDNQ (AX), R8, R8
	MOVQ  R8, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	ADDQ $0x00000008, SI
	SUBQ $0x00000001, DI
	JMP  tail

done:
	RET

// func x64or4(a []uint64, b []uint64, c []uint64, d []uint64, e []uint64)
// Requires: AVX, AVX2
TEXT ·x64or4(SB), NOSPLIT, $0-120
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ e_base+96(FP), SI
	MOVQ b_len+32(FP), DI
	XORQ R8, R8

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DI, $0x00000008
	JL   tail

	// perform the logical "OR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (SI), Y0
	VMOVUPD 32(SI), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	ADDQ $0x00000040, SI
	SUBQ $0x00000008, DI
	JMP  body

tail:
	CMPQ DI, $0x00
	JE   done

	// perform the logical "OR" operation
	MOVQ (CX), R8
	ORQ  (AX), R8
	MOVQ R8, (AX)
	MOVQ (DX), R8
	ORQ  (AX), R8
	MOVQ R8, (AX)
	MOVQ (BX), R8
	ORQ  (AX), R8
	MOVQ R8, (AX)
	MOVQ (SI), R8
	ORQ  (AX), R8
	MOVQ R8, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	ADDQ $0x00000008, SI
	SUBQ $0x00000001, DI
	JMP  tail

done:
	RET

// func x64xor4(a []uint64, b []uint64, c []uint64, d []uint64, e []uint64)
// Requires: AVX, AVX2
TEXT ·x64xor4(SB), NOSPLIT, $0-120
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ c_base+48(FP), DX
	MOVQ d_base+72(FP), BX
	MOVQ e_base+96(FP), SI
	MOVQ b_len+32(FP), DI
	XORQ R8, R8

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DI, $0x00000008
	JL   tail

	// perform the logical "XOR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (DX), Y0
	VMOVUPD 32(DX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (BX), Y0
	VMOVUPD 32(BX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)
	VMOVUPD (SI), Y0
	VMOVUPD 32(SI), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	ADDQ $0x00000040, DX
	ADDQ $0x00000040, BX
	ADDQ $0x00000040, SI
	SUBQ $0x00000008, DI
	JMP  body

tail:
	CMPQ DI, $0x00
	JE   done

	// perform the logical "XOR" operation
	MOVQ (CX), R8
	XORQ (AX), R8
	MOVQ R8, (AX)
	MOVQ (DX), R8
	XORQ (AX), R8
	MOVQ R8, (AX)
	MOVQ (BX), R8
	XORQ (AX), R8
	MOVQ R8, (AX)
	MOVQ (SI), R8
	XORQ (AX), R8
	MOVQ R8, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	ADDQ $0x00000008, DX
	ADDQ $0x00000008, BX
	ADDQ $0x00000008, SI
	SUBQ $0x00000001, DI
	JMP  tail

done:
	RET
